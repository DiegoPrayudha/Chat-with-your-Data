{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World !\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from huggingface_hub import login\n",
    "from langchain.chains import RetrievalQA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import DirectoryLoader, CSVLoader\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Groq from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "model_llm = ChatGroq(groq_api_key=api_key,model_name=\"llama-3.1-8b-instant\")\n",
    "model_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embedding Model from  Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(model_name= embedding_model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Data to VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV Files from Directory\n",
    "folder_path = 'data/used_car_dataset.csv'\n",
    "loader = CSVLoader(folder_path)\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"car_dataset\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "        You are an expert data analyst and Python programmer specializing in tax data visualization for Indonesia. Your task is to generate Python code that creates insightful plots using the Matplotlib library.\n",
    "\n",
    "        Given:\n",
    "        1. CSV file path: {df_path}\n",
    "        2. DataFrame columns: {df_columns}\n",
    "        3. User's request: \"{input}\"\n",
    "        \n",
    "        Instructions:\n",
    "        1. Analyze the user's request and the provided data structure.\n",
    "        2. Generate Python code that accomplishes the following:\n",
    "           a. Imports necessary libraries (pandas, matplotlib.pyplot).\n",
    "           b. Reads the CSV file using the provided path.\n",
    "           c. Adds appropriate labels, title, and legend to the plot.\n",
    "           d. Improves the plot's readability and aesthetics (e.g., adjusting colors, font sizes, or layout).\n",
    "        3. Ensure the code is efficient, well-commented, and follows Python best practices.\n",
    "        4. Ensure the code must simple\n",
    "        \n",
    "        Please provide only Python code.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_chart = PromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Generate Teks\n",
    "def generate_teks(query:str, model = model_llm, retriever = retriever, prompt = prompt):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "                model,\n",
    "                retriever=retriever,\n",
    "                return_source_documents=False,\n",
    "                chain_type_kwargs={\"prompt\": prompt}\n",
    "                )\n",
    "    response = qa_chain.invoke({\"query\": query})\n",
    "    return response[\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
